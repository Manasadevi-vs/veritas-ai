# src/audit_report.py

import json
import os
from typing import Dict, Any, List
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import ChatOllama

# üß† Shared prompt used by all models
AUDIT_PROMPT = ChatPromptTemplate.from_template("""
You are an AI fairness and interpretability auditor.

You are given:
1. Model metrics (accuracy, f1, precision, recall)
2. Bias summary across protected attributes
3. SHAP-based top features

Write a concise, structured audit report in Markdown including:
- Overall model performance summary
- Fairness across sensitive groups
- Interpretation of top features and risks
- 3‚Äì5 bias mitigation suggestions

Model Metrics:
{metrics_json}

Bias Summary:
{bias_json}

Top SHAP Features:
{shap_json}
""")


def _format_messages(metrics, bias_summary, shap_summary):
    return AUDIT_PROMPT.format_messages(
        metrics_json=json.dumps(metrics, indent=2),
        bias_json=json.dumps(bias_summary, indent=2),
        shap_json=json.dumps(shap_summary[:10], indent=2),
    )


def generate_llm_audit_report(metrics, bias_summary, shap_summary):
    """Generates reports from GPT-4o, Gemini, and Llama; merges into a consensus."""

    openai_key = os.getenv("OPENAI_API_KEY")
    gemini_key = os.getenv("GEMINI_API_KEY")

    messages = _format_messages(metrics, bias_summary, shap_summary)

    reports = {}
    model_sources = {}

    # --- GPT-4o
    try:
        if openai_key:
            print("üí° Using OpenAI GPT-4o")
            llm_gpt = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=openai_key)
            resp = llm_gpt.invoke(messages)
            reports["gpt4o"] = resp.content
            model_sources["gpt4o"] = "openai"
    except Exception as e:
        print(f"‚ö†Ô∏è GPT-4o failed: {e}")

    # --- Gemini
    try:
        if gemini_key:
            print("üí° Using Google Gemini 2.0 Flash")
            llm_gem = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.2)
            resp = llm_gem.invoke(messages)
            reports["gemini"] = resp.content
            model_sources["gemini"] = "gemini"
    except Exception as e:
        print(f"‚ö†Ô∏è Gemini failed: {e}")

    # --- Local Llama (always run)
    try:
        print("üí° Using Local Llama 3.2")
        llm_llama = ChatOllama(model="llama3.2:1b", temperature=0.2)
        resp = llm_llama.invoke(messages)
        reports["llama"] = resp.content
        model_sources["llama"] = "ollama"
    except Exception as e:
        print(f"‚ö†Ô∏è Llama failed: {e}")

    # --- Consensus step
    consensus_prompt = ChatPromptTemplate.from_template("""
You are a meta-AI acting as an arbiter between three audit reports generated by different AI models.

Each model has provided its fairness and interpretability analysis below.
Your task:
- Identify key agreements and disagreements
- Highlight unique insights from each model
- Produce a **final unified consensus report** in Markdown
- Ensure it's logically consistent and balanced

### GPT-4o Report:
{gpt4o}

### Gemini Report:
{gemini}

### Llama Report:
{llama}

Now write a unified, clearly structured consensus audit summary.
""")

    consensus_messages = consensus_prompt.format_messages(
        gpt4o=reports.get("gpt4o", "No response"),
        gemini=reports.get("gemini", "No response"),
        llama=reports.get("llama", "No response"),
    )

    consensus_report = "Consensus could not be generated."

    try:
        # Prefer GPT-4o for merging if available
        if openai_key:
            arbiter = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=openai_key)
            resp = arbiter.invoke(consensus_messages)
            consensus_report = resp.content
        else:
            arbiter = ChatOllama(model="llama3.2:1b", temperature=0.2)
            resp = arbiter.invoke(consensus_messages)
            consensus_report = resp.content
    except Exception as e:
        print(f"‚ö†Ô∏è Consensus generation failed: {e}")

    return {
        "individual_reports": reports,
        "consensus_report": consensus_report,
        "model_sources": model_sources,
    }
